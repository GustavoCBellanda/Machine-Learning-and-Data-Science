{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 98,4% (40 trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Machine Learning e Data Science com Python de A Ã  Z/Bases de dados/credit.pkl', 'rb') as f:\n",
    "    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=40, random_state=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_credit= RandomForestClassifier(n_estimators=40, criterion='entropy', random_state=0)\n",
    "random_forest_credit.fit(x_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = random_forest_credit.predict(x_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(random_forest_credit)\n",
    "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(x_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       436\n",
      "           1       0.95      0.92      0.94        64\n",
      "\n",
      "    accuracy                           0.98       500\n",
      "   macro avg       0.97      0.96      0.96       500\n",
      "weighted avg       0.98      0.98      0.98       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    A random forest classifier.\n",
      "\n",
      "    A random forest is a meta estimator that fits a number of decision tree\n",
      "    classifiers on various sub-samples of the dataset and uses averaging to\n",
      "    improve the predictive accuracy and control over-fitting.\n",
      "    The sub-sample size is controlled with the `max_samples` parameter if\n",
      "    `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "    each tree.\n",
      "\n",
      "    Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    n_estimators : int, default=100\n",
      "        The number of trees in the forest.\n",
      "\n",
      "        .. versionchanged:: 0.22\n",
      "           The default value of ``n_estimators`` changed from 10 to 100\n",
      "           in 0.22.\n",
      "\n",
      "    criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "        The function to measure the quality of a split. Supported criteria are\n",
      "        \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "        Note: this parameter is tree-specific.\n",
      "\n",
      "    max_depth : int, default=None\n",
      "        The maximum depth of the tree. If None, then nodes are expanded until\n",
      "        all leaves are pure or until all leaves contain less than\n",
      "        min_samples_split samples.\n",
      "\n",
      "    min_samples_split : int or float, default=2\n",
      "        The minimum number of samples required to split an internal node:\n",
      "\n",
      "        - If int, then consider `min_samples_split` as the minimum number.\n",
      "        - If float, then `min_samples_split` is a fraction and\n",
      "          `ceil(min_samples_split * n_samples)` are the minimum\n",
      "          number of samples for each split.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_samples_leaf : int or float, default=1\n",
      "        The minimum number of samples required to be at a leaf node.\n",
      "        A split point at any depth will only be considered if it leaves at\n",
      "        least ``min_samples_leaf`` training samples in each of the left and\n",
      "        right branches.  This may have the effect of smoothing the model,\n",
      "        especially in regression.\n",
      "\n",
      "        - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "        - If float, then `min_samples_leaf` is a fraction and\n",
      "          `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "          number of samples for each node.\n",
      "\n",
      "        .. versionchanged:: 0.18\n",
      "           Added float values for fractions.\n",
      "\n",
      "    min_weight_fraction_leaf : float, default=0.0\n",
      "        The minimum weighted fraction of the sum total of weights (of all\n",
      "        the input samples) required to be at a leaf node. Samples have\n",
      "        equal weight when sample_weight is not provided.\n",
      "\n",
      "    max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "        The number of features to consider when looking for the best split:\n",
      "\n",
      "        - If int, then consider `max_features` features at each split.\n",
      "        - If float, then `max_features` is a fraction and\n",
      "          `round(max_features * n_features)` features are considered at each\n",
      "          split.\n",
      "        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "        - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      "        - If \"log2\", then `max_features=log2(n_features)`.\n",
      "        - If None, then `max_features=n_features`.\n",
      "\n",
      "        Note: the search for a split does not stop until at least one\n",
      "        valid partition of the node samples is found, even if it requires to\n",
      "        effectively inspect more than ``max_features`` features.\n",
      "\n",
      "    max_leaf_nodes : int, default=None\n",
      "        Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "        Best nodes are defined as relative reduction in impurity.\n",
      "        If None then unlimited number of leaf nodes.\n",
      "\n",
      "    min_impurity_decrease : float, default=0.0\n",
      "        A node will be split if this split induces a decrease of the impurity\n",
      "        greater than or equal to this value.\n",
      "\n",
      "        The weighted impurity decrease equation is the following::\n",
      "\n",
      "            N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                                - N_t_L / N_t * left_impurity)\n",
      "\n",
      "        where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "        samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "        left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "        ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "        if ``sample_weight`` is passed.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "\n",
      "    bootstrap : bool, default=True\n",
      "        Whether bootstrap samples are used when building trees. If False, the\n",
      "        whole dataset is used to build each tree.\n",
      "\n",
      "    oob_score : bool, default=False\n",
      "        Whether to use out-of-bag samples to estimate the generalization score.\n",
      "        Only available if bootstrap=True.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "        :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "        trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "        context. ``-1`` means using all processors. See :term:`Glossary\n",
      "        <n_jobs>` for more details.\n",
      "\n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls both the randomness of the bootstrapping of the samples used\n",
      "        when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "        features to consider when looking for the best split at each node\n",
      "        (if ``max_features < n_features``).\n",
      "        See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        Controls the verbosity when fitting and predicting.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to ``True``, reuse the solution of the previous call to fit\n",
      "        and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "        new forest. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "    class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If not given, all classes are supposed to have weight one. For\n",
      "        multi-output problems, a list of dicts can be provided in the same\n",
      "        order as the columns of y.\n",
      "\n",
      "        Note that for multioutput (including multilabel) weights should be\n",
      "        defined for each class of every column in its own dict. For example,\n",
      "        for four-class multilabel classification weights should be\n",
      "        [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "        [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``\n",
      "\n",
      "        The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "        weights are computed based on the bootstrap sample for every tree\n",
      "        grown.\n",
      "\n",
      "        For multi-output, the weights of each column of y will be multiplied.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "    ccp_alpha : non-negative float, default=0.0\n",
      "        Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "        subtree with the largest cost complexity that is smaller than\n",
      "        ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "        :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    max_samples : int or float, default=None\n",
      "        If bootstrap is True, the number of samples to draw from X\n",
      "        to train each base estimator.\n",
      "\n",
      "        - If None (default), then draw `X.shape[0]` samples.\n",
      "        - If int, then draw `max_samples` samples.\n",
      "        - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "          `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      "\n",
      "        .. versionadded:: 0.22\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    base_estimator_ : DecisionTreeClassifier\n",
      "        The child estimator template used to create the collection of fitted\n",
      "        sub-estimators.\n",
      "\n",
      "    estimators_ : list of DecisionTreeClassifier\n",
      "        The collection of fitted sub-estimators.\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "        The classes labels (single output problem), or a list of arrays of\n",
      "        class labels (multi-output problem).\n",
      "\n",
      "    n_classes_ : int or list\n",
      "        The number of classes (single output problem), or a list containing the\n",
      "        number of classes for each output (multi-output problem).\n",
      "\n",
      "    n_features_ : int\n",
      "        The number of features when ``fit`` is performed.\n",
      "\n",
      "        .. deprecated:: 1.0\n",
      "            Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      "            removed in 1.2. Use `n_features_in_` instead.\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_outputs_ : int\n",
      "        The number of outputs when ``fit`` is performed.\n",
      "\n",
      "    feature_importances_ : ndarray of shape (n_features,)\n",
      "        The impurity-based feature importances.\n",
      "        The higher, the more important the feature.\n",
      "        The importance of a feature is computed as the (normalized)\n",
      "        total reduction of the criterion brought by that feature.  It is also\n",
      "        known as the Gini importance.\n",
      "\n",
      "        Warning: impurity-based feature importances can be misleading for\n",
      "        high cardinality features (many unique values). See\n",
      "        :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "    oob_score_ : float\n",
      "        Score of the training dataset obtained using an out-of-bag estimate.\n",
      "        This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "    oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
      "        Decision function computed with out-of-bag estimate on the training\n",
      "        set. If n_estimators is small it might be possible that a data point\n",
      "        was never left out during the bootstrap. In this case,\n",
      "        `oob_decision_function_` might contain NaN. This attribute exists\n",
      "        only when ``oob_score`` is True.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "    sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
      "        tree classifiers.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The default values for the parameters controlling the size of the trees\n",
      "    (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "    unpruned trees which can potentially be very large on some data sets. To\n",
      "    reduce memory consumption, the complexity and size of the trees should be\n",
      "    controlled by setting those parameter values.\n",
      "\n",
      "    The features are always randomly permuted at each split. Therefore,\n",
      "    the best found split may vary, even with the same training data,\n",
      "    ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "    of the criterion is identical for several splits enumerated during the\n",
      "    search of the best split. To obtain a deterministic behaviour during\n",
      "    fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "    .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.ensemble import RandomForestClassifier\n",
      "    >>> from sklearn.datasets import make_classification\n",
      "    >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "    ...                            n_informative=2, n_redundant=0,\n",
      "    ...                            random_state=0, shuffle=False)\n",
      "    >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "    >>> clf.fit(X, y)\n",
      "    RandomForestClassifier(...)\n",
      "    >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "    [1]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(RandomForestClassifier.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n",
      "(2000, 3)\n",
      "{'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "0.985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "x_credit = np.concatenate((x_credit_treinamento, x_credit_teste), axis=0)\n",
    "y_credit = np.concatenate((y_credit_treinamento, y_credit_teste), axis=0)\n",
    "print(x_credit.shape)\n",
    "print(x_credit.shape)\n",
    "parameters = {'criterion': ['gini', 'entropy'],\n",
    "              'n_estimators': [10, 40, 100, 150],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 5, 10]}\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(best_parameters)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion = 'entropy', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 40\n"
     ]
    }
   ],
   "source": [
    "def prepare_tree_parameters(parameters):\n",
    "    equal_or_comma = ','\n",
    "    converted_parameters = ''\n",
    "    for i in range(len(parameters)):\n",
    "        if equal_or_comma == ',':\n",
    "            if parameters[i] == \"'\":\n",
    "                converted_parameters += ''\n",
    "            elif parameters[i] == \":\":\n",
    "                converted_parameters += ' ='\n",
    "                equal_or_comma = '='\n",
    "            else:\n",
    "                converted_parameters += parameters[i]\n",
    "        elif equal_or_comma == '=': \n",
    "            if parameters[i] == \"'\":\n",
    "                converted_parameters += parameters[i]\n",
    "            elif parameters[i] == \",\":\n",
    "                converted_parameters += ','\n",
    "                equal_or_comma = ','\n",
    "            else:\n",
    "                converted_parameters += parameters[i]\n",
    "                \n",
    "            \n",
    "    return converted_parameters\n",
    "\n",
    "parameters = prepare_tree_parameters(\"'criterion': 'entropy', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 40\")\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.986,\n",
       " 0.985,\n",
       " 0.9869999999999999,\n",
       " 0.986,\n",
       " 0.9879999999999999,\n",
       " 0.9884999999999999,\n",
       " 0.9869999999999999,\n",
       " 0.9904999999999999,\n",
       " 0.9869999999999999,\n",
       " 0.9869999999999998,\n",
       " 0.9855,\n",
       " 0.9890000000000001,\n",
       " 0.9865,\n",
       " 0.9875,\n",
       " 0.9849999999999998,\n",
       " 0.9844999999999999,\n",
       " 0.986,\n",
       " 0.9904999999999999,\n",
       " 0.9904999999999999,\n",
       " 0.986,\n",
       " 0.986,\n",
       " 0.9865,\n",
       " 0.9854999999999998,\n",
       " 0.9889999999999999,\n",
       " 0.9890000000000001,\n",
       " 0.9884999999999999,\n",
       " 0.9870000000000001,\n",
       " 0.9834999999999999,\n",
       " 0.9844999999999999,\n",
       " 0.986]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "results = []\n",
    "\n",
    "for i in range(30): \n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=i)\n",
    "    tool = RandomForestClassifier(criterion = 'entropy', min_samples_leaf = 1, min_samples_split = 2, n_estimators = 40)\n",
    "    scores = cross_val_score(tool, x_credit, y_credit, cv = kfold)\n",
    "    # print(scores)\n",
    "    results.append(scores.mean())\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50f9dc88fbf78228e28ca27dec02a2c0a62082556f09da821a5db223c4e74185"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
