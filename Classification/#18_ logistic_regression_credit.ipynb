{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 94,6%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./Machine Learning e Data Science com Python de A Ã  Z/Bases de dados/credit.pkl', 'rb') as f:\n",
    "    x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_credit = LogisticRegression(random_state=1)\n",
    "logistic_credit.fit(x_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.02976095])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_credit.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.54927091, -3.72279861,  3.93940349]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_credit.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = logistic_credit.predict(x_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(logistic_credit)\n",
    "cm.fit(x_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(x_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       436\n",
      "           1       0.79      0.78      0.79        64\n",
      "\n",
      "    accuracy                           0.95       500\n",
      "   macro avg       0.88      0.88      0.88       500\n",
      "weighted avg       0.95      0.95      0.95       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
      "\n",
      "    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      "    scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      "    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      "    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      "    'sag', 'saga' and 'newton-cg' solvers.)\n",
      "\n",
      "    This class implements regularized logistic regression using the\n",
      "    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      "    that regularization is applied by default**. It can handle both dense\n",
      "    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      "    floats for optimal performance; any other input format will be converted\n",
      "    (and copied).\n",
      "\n",
      "    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      "    with primal formulation, or no regularization. The 'liblinear' solver\n",
      "    supports both L1 and L2 regularization, with a dual formulation only for\n",
      "    the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      "    'saga' solver.\n",
      "\n",
      "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      "        Specify the norm of the penalty:\n",
      "\n",
      "        - `'none'`: no penalty is added;\n",
      "        - `'l2'`: add a L2 penalty term and it is the default choice;\n",
      "        - `'l1'`: add a L1 penalty term;\n",
      "        - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
      "\n",
      "        .. warning::\n",
      "           Some penalties may not work with some solvers. See the parameter\n",
      "           `solver` below, to know the compatibility between the penalty and\n",
      "           solver.\n",
      "\n",
      "        .. versionadded:: 0.19\n",
      "           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      "\n",
      "    dual : bool, default=False\n",
      "        Dual or primal formulation. Dual formulation is only implemented for\n",
      "        l2 penalty with liblinear solver. Prefer dual=False when\n",
      "        n_samples > n_features.\n",
      "\n",
      "    tol : float, default=1e-4\n",
      "        Tolerance for stopping criteria.\n",
      "\n",
      "    C : float, default=1.0\n",
      "        Inverse of regularization strength; must be a positive float.\n",
      "        Like in support vector machines, smaller values specify stronger\n",
      "        regularization.\n",
      "\n",
      "    fit_intercept : bool, default=True\n",
      "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
      "        added to the decision function.\n",
      "\n",
      "    intercept_scaling : float, default=1\n",
      "        Useful only when the solver 'liblinear' is used\n",
      "        and self.fit_intercept is set to True. In this case, x becomes\n",
      "        [x, self.intercept_scaling],\n",
      "        i.e. a \"synthetic\" feature with constant value equal to\n",
      "        intercept_scaling is appended to the instance vector.\n",
      "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      "\n",
      "        Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      "        as all other features.\n",
      "        To lessen the effect of regularization on synthetic feature weight\n",
      "        (and therefore on the intercept) intercept_scaling has to be increased.\n",
      "\n",
      "    class_weight : dict or 'balanced', default=None\n",
      "        Weights associated with classes in the form ``{class_label: weight}``.\n",
      "        If not given, all classes are supposed to have weight one.\n",
      "\n",
      "        The \"balanced\" mode uses the values of y to automatically adjust\n",
      "        weights inversely proportional to class frequencies in the input data\n",
      "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
      "\n",
      "        Note that these weights will be multiplied with sample_weight (passed\n",
      "        through the fit method) if sample_weight is specified.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *class_weight='balanced'*\n",
      "\n",
      "    random_state : int, RandomState instance, default=None\n",
      "        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      "        data. See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      "\n",
      "        Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
      "        To choose a solver, you might want to consider the following aspects:\n",
      "\n",
      "            - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
      "              and 'saga' are faster for large ones;\n",
      "            - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
      "              'lbfgs' handle multinomial loss;\n",
      "            - 'liblinear' is limited to one-versus-rest schemes.\n",
      "\n",
      "        .. warning::\n",
      "           The choice of the algorithm depends on the penalty chosen:\n",
      "           Supported penalties by solver:\n",
      "\n",
      "           - 'newton-cg'   -   ['l2', 'none']\n",
      "           - 'lbfgs'       -   ['l2', 'none']\n",
      "           - 'liblinear'   -   ['l1', 'l2']\n",
      "           - 'sag'         -   ['l2', 'none']\n",
      "           - 'saga'        -   ['elasticnet', 'l1', 'l2', 'none']\n",
      "\n",
      "        .. note::\n",
      "           'sag' and 'saga' fast convergence is only guaranteed on\n",
      "           features with approximately the same scale. You can\n",
      "           preprocess the data with a scaler from :mod:`sklearn.preprocessing`.\n",
      "\n",
      "        .. seealso::\n",
      "           Refer to the User Guide for more information regarding\n",
      "           :class:`LogisticRegression` and more specifically the\n",
      "           `Table <https://scikit-learn.org/dev/modules/linear_model.html#logistic-regression>`_\n",
      "           summarazing solver/penalty supports.\n",
      "           <!--\n",
      "           # noqa: E501\n",
      "           -->\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           Stochastic Average Gradient descent solver.\n",
      "        .. versionadded:: 0.19\n",
      "           SAGA solver.\n",
      "        .. versionchanged:: 0.22\n",
      "            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      "\n",
      "    max_iter : int, default=100\n",
      "        Maximum number of iterations taken for the solvers to converge.\n",
      "\n",
      "    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
      "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      "        across the entire probability distribution, *even when the data is\n",
      "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      "        and otherwise selects 'multinomial'.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "           Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      "        .. versionchanged:: 0.22\n",
      "            Default changed from 'ovr' to 'auto' in 0.22.\n",
      "\n",
      "    verbose : int, default=0\n",
      "        For the liblinear and lbfgs solvers set verbose to any positive\n",
      "        number for verbosity.\n",
      "\n",
      "    warm_start : bool, default=False\n",
      "        When set to True, reuse the solution of the previous call to fit as\n",
      "        initialization, otherwise, just erase the previous solution.\n",
      "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "        .. versionadded:: 0.17\n",
      "           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      "\n",
      "    n_jobs : int, default=None\n",
      "        Number of CPU cores used when parallelizing over classes if\n",
      "        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      "        set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      "        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "        context. ``-1`` means using all processors.\n",
      "        See :term:`Glossary <n_jobs>` for more details.\n",
      "\n",
      "    l1_ratio : float, default=None\n",
      "        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      "        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      "        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      "        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      "        combination of L1 and L2.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "\n",
      "    classes_ : ndarray of shape (n_classes, )\n",
      "        A list of class labels known to the classifier.\n",
      "\n",
      "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      "        Coefficient of the features in the decision function.\n",
      "\n",
      "        `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      "        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      "        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      "\n",
      "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      "        Intercept (a.k.a. bias) added to the decision function.\n",
      "\n",
      "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
      "        `intercept_` is of shape (1,) when the given problem is binary.\n",
      "        In particular, when `multi_class='multinomial'`, `intercept_`\n",
      "        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      "        outcome 0 (False).\n",
      "\n",
      "    n_features_in_ : int\n",
      "        Number of features seen during :term:`fit`.\n",
      "\n",
      "        .. versionadded:: 0.24\n",
      "\n",
      "    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      "        Names of features seen during :term:`fit`. Defined only when `X`\n",
      "        has feature names that are all strings.\n",
      "\n",
      "        .. versionadded:: 1.0\n",
      "\n",
      "    n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      "        Actual number of iterations for all classes. If binary or multinomial,\n",
      "        it returns only 1 element. For liblinear solver, only the maximum\n",
      "        number of iteration across all classes is given.\n",
      "\n",
      "        .. versionchanged:: 0.20\n",
      "\n",
      "            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      "            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    SGDClassifier : Incrementally trained logistic regression (when given\n",
      "        the parameter ``loss=\"log\"``).\n",
      "    LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    The underlying C implementation uses a random number generator to\n",
      "    select features when fitting the model. It is thus not uncommon,\n",
      "    to have slightly different results for the same input data. If\n",
      "    that happens, try with a smaller tol parameter.\n",
      "\n",
      "    Predict output may not match that of standalone liblinear in certain\n",
      "    cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      "    in the narrative documentation.\n",
      "\n",
      "    References\n",
      "    ----------\n",
      "\n",
      "    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      "        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      "        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      "\n",
      "    LIBLINEAR -- A Library for Large Linear Classification\n",
      "        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      "\n",
      "    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      "        Minimizing Finite Sums with the Stochastic Average Gradient\n",
      "        https://hal.inria.fr/hal-00860051/document\n",
      "\n",
      "    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      "        SAGA: A Fast Incremental Gradient Method With Support\n",
      "        for Non-Strongly Convex Composite Objectives\n",
      "        https://arxiv.org/abs/1407.0202\n",
      "\n",
      "    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      "        methods for logistic regression and maximum entropy models.\n",
      "        Machine Learning 85(1-2):41-75.\n",
      "        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_iris\n",
      "    >>> from sklearn.linear_model import LogisticRegression\n",
      "    >>> X, y = load_iris(return_X_y=True)\n",
      "    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      "    >>> clf.predict(X[:2, :])\n",
      "    array([0, 0])\n",
      "    >>> clf.predict_proba(X[:2, :])\n",
      "    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      "           [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      "    >>> clf.score(X, y)\n",
      "    0.97...\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(LogisticRegression.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 3)\n",
      "(2000, 3)\n",
      "{'C': 1.0, 'solver': 'newton-cg', 'tol': 0.001}\n",
      "0.9484999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "x_credit = np.concatenate((x_credit_treinamento, x_credit_teste), axis=0)\n",
    "y_credit = np.concatenate((y_credit_treinamento, y_credit_teste), axis=0)\n",
    "print(x_credit.shape)\n",
    "print(x_credit.shape)\n",
    "parameters = {'tol': [0.001, 0.0001, 0.00001],\n",
    "              'C': [1.0, 1.5, 2.0],\n",
    "              'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],}\n",
    "grid_search = GridSearchCV(estimator=LogisticRegression(), param_grid=parameters)\n",
    "grid_search.fit(x_credit, y_credit)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(best_parameters)\n",
    "print(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1.0, solver = 'newton-cg', tol = 0.001\n"
     ]
    }
   ],
   "source": [
    "def prepare_tree_parameters(parameters):\n",
    "    equal_or_comma = ','\n",
    "    converted_parameters = ''\n",
    "    for i in range(len(parameters)):\n",
    "        if equal_or_comma == ',':\n",
    "            if parameters[i] == \"'\":\n",
    "                converted_parameters += ''\n",
    "            elif parameters[i] == \":\":\n",
    "                converted_parameters += ' ='\n",
    "                equal_or_comma = '='\n",
    "            else:\n",
    "                converted_parameters += parameters[i]\n",
    "        elif equal_or_comma == '=': \n",
    "            if parameters[i] == \"'\":\n",
    "                converted_parameters += parameters[i]\n",
    "            elif parameters[i] == \",\":\n",
    "                converted_parameters += ','\n",
    "                equal_or_comma = ','\n",
    "            else:\n",
    "                converted_parameters += parameters[i]\n",
    "                \n",
    "            \n",
    "    return converted_parameters\n",
    "\n",
    "parameters = prepare_tree_parameters(\"'C': 1.0, 'solver': 'newton-cg', 'tol': 0.001\")\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "results = []\n",
    "\n",
    "for i in range(30): \n",
    "    kfold = KFold(n_splits=10, shuffle=True, random_state=i)\n",
    "    tool = LogisticRegression(C = 1.0, solver = 'newton-cg', tol = 0.001)\n",
    "    scores = cross_val_score(tool, x_credit, y_credit, cv = kfold)\n",
    "    # print(scores)\n",
    "    results.append(scores.mean())\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50f9dc88fbf78228e28ca27dec02a2c0a62082556f09da821a5db223c4e74185"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
